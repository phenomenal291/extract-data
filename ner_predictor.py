#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple Vietnamese NER Prediction
Chá»‰ cÃ³ function cÆ¡ báº£n Ä‘á»ƒ dá»± Ä‘oÃ¡n NER tá»« model Ä‘Ã£ train
"""

import torch
import re
from transformers import AutoTokenizer, AutoModelForTokenClassification
from typing import List, Dict


class VietnameseNERPredictor:
    """Simple Vietnamese NER Predictor"""
    
    def __init__(self, model_path: str = "./mdeberta_ner_model/final"):
        """Initialize predictor with model"""
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(model_path)
        self.model.eval()
        
        # Move to GPU if available
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print("Model loaded on GPU")
        else:
            print("Model loaded on CPU")
    
    def smart_chunk_text(self, text: str, max_length: int = 400) -> List[str]:
        chunks = []
        hashtag_pattern = r'#\w+'
        hashtags = re.findall(hashtag_pattern, text)
        text_without_hashtags = re.sub(hashtag_pattern, '', text).strip()
        
        lines = text_without_hashtags.split('\n')
        current_chunk = ""
        contact_block = ""
        in_contact_block = False
        contact_keywords = ['hotline', 'Ä‘á»‹a chá»‰', 'website', 'email', 'zalo', 'facebook', 'tel', 'fax', 'tá»•ng Ä‘Ã i']
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if not line:
                i += 1
                continue
                
            line_lower = line.lower()
            is_contact_line = any(keyword in line_lower for keyword in contact_keywords)
            has_address = bool(re.search(r'(sá»‘\s+\d+|Ä‘Æ°á»ng|phÆ°á»ng|quáº­n|tá»‰nh|thÃ nh phá»‘)', line_lower))
            has_org_pattern = bool(re.search(r'<ORG>.*?</ORG>', line))
            has_addr_pattern = bool(re.search(r'<ADDR>.*?</ADDR>', line))
            
            if is_contact_line or has_address or has_org_pattern or has_addr_pattern:
                if not in_contact_block:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        current_chunk = ""
                    in_contact_block = True
                    contact_block = line
                else:
                    contact_block += " " + line
            else:
                if in_contact_block:
                    chunks.append(contact_block.strip())
                    contact_block = ""
                    in_contact_block = False
                
                if re.match(r'^[\-\*]\s+', line) or re.match(r'^\d+[\.\)]\s+', line):
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        current_chunk = ""
                    
                    list_item = line
                    j = i + 1
                    while j < len(lines) and lines[j].strip():
                        next_line = lines[j].strip()
                        if not (re.match(r'^[\-\*]\s+', next_line) or re.match(r'^\d+[\.\)]\s+', next_line)):
                            next_line_lower = next_line.lower()
                            is_next_contact = any(keyword in next_line_lower for keyword in contact_keywords)
                            
                            if not is_next_contact:
                                list_item += " " + next_line
                                j += 1
                            else:
                                break
                        else:
                            break
                    
                    chunks.append(list_item.strip())
                    i = j - 1
                else:
                    if len(current_chunk) + len(line) + 1 <= max_length:
                        if current_chunk:
                            current_chunk += " " + line
                        else:
                            current_chunk = line
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = line
            
            i += 1
        
        if in_contact_block and contact_block:
            chunks.append(contact_block.strip())
        elif current_chunk:
            chunks.append(current_chunk.strip())
        
        if hashtags and chunks:
            hashtag_text = " " + " ".join(hashtags)
            chunks[-1] += hashtag_text
        
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= max_length:
                final_chunks.append(chunk)
            else:
                sentences = re.split(r'[.!?]\s+|[.!?]$|(?<=\.)\s+(?=[A-ZÃ€ÃÃ‚ÃƒÃˆÃ‰ÃŠÃŒÃÃ’Ã“Ã”Ã•Ã™ÃšÄ‚ÄÄ¨Å¨Æ Æ¯])', chunk)
                
                current_subchunk = ""
                for sent in sentences:
                    sent = sent.strip()
                    if not sent:
                        continue
                        
                    if len(current_subchunk) + len(sent) + 2 <= max_length:
                        if current_subchunk:
                            current_subchunk += ". " + sent
                        else:
                            current_subchunk = sent
                    else:
                        if current_subchunk:
                            final_chunks.append(current_subchunk.strip())
                        
                        if len(sent) > max_length:
                            clauses = re.split(r',\s+|;\s+|\s+vÃ \s+|\s+hoáº·c\s+|\s+nhÆ°ng\s+|\s+mÃ \s+', sent)
                            current_subchunk = ""
                            for clause in clauses:
                                clause = clause.strip()
                                if len(current_subchunk) + len(clause) + 2 <= max_length:
                                    if current_subchunk:
                                        current_subchunk += ", " + clause
                                    else:
                                        current_subchunk = clause
                                else:
                                    if current_subchunk:
                                        final_chunks.append(current_subchunk.strip())
                                    current_subchunk = clause
                        else:
                            current_subchunk = sent
                
                if current_subchunk:
                    final_chunks.append(current_subchunk.strip())
        
        return [chunk for chunk in final_chunks if chunk.strip()]

    def predict_text(self, text: str) -> Dict:
        """
        Dá»± Ä‘oÃ¡n entities trong vÄƒn báº£n
        
        Args:
            text: VÄƒn báº£n cáº§n dá»± Ä‘oÃ¡n
            
        Returns:
            Dictionary chá»©a káº¿t quáº£ dá»± Ä‘oÃ¡n
        """
        chunks = self.smart_chunk_text(text, max_length=400)
        all_entities = []
        
        current_offset = 0
        for chunk in chunks:
            chunk_entities = self._predict_chunk(chunk)
            
            chunk_start = text.find(chunk, current_offset)
            if chunk_start != -1:
                for entity in chunk_entities:
                    entity["start"] += chunk_start
                    entity["end"] += chunk_start
                    all_entities.append(entity)
            
            current_offset = text.find(chunk, current_offset) + len(chunk)
        
        entities_by_type = {}
        for entity in all_entities:
            label = entity["label"]
            if label not in entities_by_type:
                entities_by_type[label] = []
            entities_by_type[label].append(entity)
        
        return {
            "text": text,
            "entities": all_entities,
            "entities_by_type": entities_by_type,
            "total_entities": len(all_entities),
            "entity_types": list(entities_by_type.keys())
        }

    def _predict_chunk(self, text: str) -> List[Dict]:
        # Tokenize
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            return_offsets_mapping=True
        )
        
        offset_mapping = inputs.pop("offset_mapping")
        
        # Move to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = outputs.logits.argmax(dim=2)
            confidences = torch.softmax(outputs.logits, dim=2).max(dim=2)[0]
        
        # Convert to CPU
        predictions = predictions.cpu().numpy()[0]
        confidences = confidences.cpu().numpy()[0]
        offset_mapping = offset_mapping.cpu().numpy()[0]
        
        # Extract entities
        entities = []
        current_entity = None
        
        for i, (pred_id, conf, (start, end)) in enumerate(zip(predictions, confidences, offset_mapping)):
            if start == end:  # Skip special tokens
                continue
                
            label = self.model.config.id2label[int(pred_id)]
            
            if label == "O":
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
                continue
            
            entity_type = label[2:]  # Remove B- or I-
            
            if label.startswith("B-"):
                if current_entity:
                    entities.append(current_entity)
                
                current_entity = {
                    "text": text[start:end],
                    "label": entity_type,
                    "start": int(start),
                    "end": int(end),
                    "confidence": float(conf)
                }
            elif label.startswith("I-") and current_entity and current_entity["label"] == entity_type:
                current_entity["text"] += text[start:end]
                current_entity["end"] = int(end)
                current_entity["confidence"] = max(current_entity["confidence"], float(conf))
        
        if current_entity:
            entities.append(current_entity)
        
        return entities

    def format_results(self, results: Dict, show_confidence: bool = True) -> str:
        """Format káº¿t quáº£ Ä‘á»ƒ hiá»ƒn thá»‹"""
        output = []
        output.append(f"ğŸ“ Text: {results['text'][:100]}{'...' if len(results['text']) > 100 else ''}")
        output.append(f"ğŸ“Š Found {results['total_entities']} entities")
        output.append("")
        
        if results['entities']:
            output.append("ğŸ·ï¸ Detected Entities:")
            for i, entity in enumerate(results['entities'], 1):
                confidence_str = f" (confidence: {entity['confidence']:.3f})" if show_confidence else ""
                output.append(f"   {i}. '{entity['text']}' â†’ {entity['label']}{confidence_str}")
            
            output.append("")
            output.append("ğŸ“‹ By Category:")
            for entity_type in sorted(results['entities_by_type'].keys()):
                entities = results['entities_by_type'][entity_type]
                output.append(f"   {entity_type}: {len(entities)} entities")
                for entity in entities:
                    output.append(f"      â€¢ {entity['text']}")
        else:
            output.append("âŒ No entities detected")
        
        return "\n".join(output)

def main():
    """Test function Ä‘Æ¡n giáº£n"""
    # Simple test
    predictor = VietnameseNERPredictor()
    
    test_text ="""[MEGA LIVE 14.11] DEAL Äá»ˆNH THÃŒNH LÃŒNH - LÃ€M Äáº¸P SIÃŠU DÃNH CÃ¹ng Ca sÄ© THU THá»¦Y SÄƒn deal Äá»’NG GIÃ tá»« 99K - QuÃ  táº·ng LÃ m Ä‘áº¹p siÃªu khá»§ng----------Danh sÃ¡ch cÃ¡c dá»‹ch vá»¥ siÃªu Hot sáº½ xuáº¥t hiá»‡n trong phiÃªn live láº§n nÃ y, vá»›i má»©c giÃ¡ Äá»˜C QUYá»€N siÃªu giáº£m:- ChÄƒm SÃ³c Da Cao Cáº¥p 3in1- Dr.Vip ChÄƒm SÃ³c Da LÃ£o HoÃ¡ ECM- Dr.Vip á»¦ Tráº¯ng Face Collagen- Dr.Vip ChÄƒm SÃ³c VÃ¹ng Máº¯t ECM - XoÃ¡ nhÄƒn váº¿t chÃ¢n chim- Dr.Vip Collagen Thuá»· PhÃ¢n - á»¨c Cháº¿ Äá»‘m NÃ¢u- Dr. Acne Trá»‹ Má»¥n Chuáº©n Y Khoa- Dr.Seoul Laser Pico 5.0- Dr.Slim Giáº£m Má»¡ Exilis Detox- Dr. White Táº¯m Tráº¯ng HoÃ ng Gia- Phun mÃ y- Phun mÃ­- Phun mÃ´iNgoÃ i ra, cÃ¡c hoáº¡t Ä‘á»™ng cá»™ng hÆ°á»Ÿng táº¡i phiÃªn live: Giao lÆ°u, trÃ² chuyá»‡n, chia sáº» kiáº¿n thá»©c lÃ m Ä‘áº¹p cÃ¹ng ca sÄ© Thu Thá»§y TÆ° váº¥n & giáº£i Ä‘Ã¡p vá» dá»‹ch vá»¥ cÃ¹ng Seoul Center Tham gia minigame - Nháº­n quÃ  Ä‘á»™c quyá»n thÆ°Æ¡ng hiá»‡uTáº¥t cáº£ DEAL há»i Ä‘Ã£ sáºµn sÃ ng "lÃªn ká»‡" vÃ o lÃºc 19h00 | 14.11.2024 táº¡i FB/ Tiktok Seoul Center vÃ  Fb/tiktok ca sÄ© Thu Thá»§y Giáº£m giÃ¡ ká»‹ch sÃ n, chá»‰ cÃ³ trÃªn live Äáº·t lá»‹ch sÄƒn ngay lÃ m Ä‘áº¹p Ä‘Ã³n táº¿t cÃ¹ng Thu Thá»§y nhÃ©!-------------Há»‡ Thá»‘ng Tháº©m Má»¹ Quá»‘c Táº¿ Seoul CenterSáºµn sÃ ng láº¯ng nghe má»i Ã½ kiáº¿n cá»§a khÃ¡ch hÃ ng: 1800 3333Äáº·t lá»‹ch ngay vá»›i Top dá»‹ch vá»¥ Ä‘áº·c quyá»n: Website: Zalo: Tiktok: Youtube: Top 10 ThÆ°Æ¡ng Hiá»‡u Xuáº¥t Sáº¯c ChÃ¢u Ã 2022 & 2023Huy ChÆ°Æ¡ng VÃ ng Sáº£n Pháº©m, Dá»‹ch Vá»¥ Cháº¥t LÆ°á»£ng ChÃ¢u Ã 2023ThÆ°Æ¡ng Hiá»‡u Tháº©m Má»¹ Dáº«n Äáº§u Viá»‡t Nam 2024SEOUL CENTER - PHá»¤NG Sá»° Tá»ª TÃ‚M#SeoulCenter #ThamMyVien"""
    
    results = predictor.predict_text(test_text)
    print(predictor.format_results(results))


if __name__ == "__main__":
    main()